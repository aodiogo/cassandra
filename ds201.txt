dse cassandra

/home/ubuntu/node/resources/cassandra/bin/cqlsh

dsetool status

CREATE KEYSPACE killrvideo
WITH replication = {
'class':'SimpleStrategy',
'replication_factor': 1
};

USE killrvideo;

CREATE TABLE videos (
video_id TIMEUUID,
added_date TIMESTAMP,
title TEXT,
PRIMARY KEY (video_id)
);


INSERT INTO videos (video_id, added_date, title)
VALUES (1645ea59-14bd-11e5-a993-8138354b7e31, '2014-01-29', 'Cassandra History');

TRUNCATE videos;


COPY videos(video_id, added_date, title)
FROM '/home/ubuntu/labwork/data-files/videos.csv'
WITH HEADER=TRUE;

/home/ubuntu/node/bin/dsetool status

SELECT token(video_id), video_id
FROM videos;

CREATE TABLE videos_by_tag (
tag TEXT,
video_id UUID,
added_date TIMESTAMP,
title TEXT,
PRIMARY KEY ((tag), video_id)
);

CREATE TABLE videos_by_tag (
tag text,
video_id uuid,
added_date timestamp,
title text,
PRIMARY KEY ((tag), added_date, video_id)
) WITH CLUSTERING ORDER BY(added_date DESC) WITH HEADER = true;

SELECT *
FROM videos_by_tag
ORDER BY added_date ASC;

SELECT *
FROM videos_by_tag
WHERE tag = 'cassandra'
ORDER BY added_date ASC;

SELECT *
FROM videos_by_tag
WHERE tag = 'cassandra' and added_date >= '2013-1-1'
ORDER BY added_date ASC;

# python

#start python interpreter
python

print('{0:12} {1:40} {2:5}'.format('Tag', 'ID', 'Title'))
for val in session.execute("select * from videos_by_tag"):
 print('{0:12} {1:40} {2:5}'.format(val[0], val[2], val[3]))

print('{0:12} {1:40} {2:5}'.format('Tag', 'ID', 'Title'))
for val in session.execute("select * from videos_by_tag"):
 print('{0:12} {1:40} {2:5}'.format(val[0], val[2], val[3]))

nodetool help

nodetool status

nodetool info

nodetool describecluster

nodetool getlogginglevels

./nodetool setlogginglevel org.apache.cassandra TRACE

The command setlogginglevel dynamically changes the logging level used by
Apache Cassandra™ without the need for a restart. You can also look at the
/var/log/cassandra/system.log afterwards to observe the changes.

./nodetool settraceprobability 0.1

The resultant value from the settraceprobability command represents a decimal
describing the percentage of queries being saved, starting from 0 (0%) to 1 (100%). Saved traces
can then be viewed in the system_traces keyspace.

./nodetool drain
The drain command stops writes from occurring on the node and flushes all data to disk.
Typically, this command may be run before stopping an Apache Cassandra™ node.

./nodetool stopdaemon
The stopdaemon command stops a node's execution. Wait for it to complete.

Restart node by running:

/home/ubuntu/node/bin/dse cassandra

We will now stress the node using a simple tool called Apache Cassandra(TM) Stress. Once
your node has restarted, navigate to the
/home/ubuntu/node/resources/cassandra/tools/bin directory in the terminal. Run
cassandra-stress to populate the cluster with 50,000 partitions using 1 client thread and
without any warmup using:
./cassandra-stress write n=50000 no-warmup -rate threads=1


nodetool flush

Initially, we will see a long list of setting for the stress run. As Apache Cassandra™ stress
executes, it logs several statistics to the terminal. Each line displays the statistics for the
operations that occurred each second and shows number of partitions written, operations per
second, latency information, and more.

The flush command commits all written (memtable, discussed later) data to disk. Unlike
drain, flush allows further writes to occur.

inside cqlsh:

DESCRIBE KEYSPACES;

USE keyspace1;

DESCRIBE TABLES;

SELECT *
FROM standard1
LIMIT 5;

Stop the current node
nodetool stopdaemon 

Ring

/home/ubuntu/node2/resources/cassandra/conf/cassandra.yaml

Change initial token to

initial_token: 9223372036854775807

/home/ubuntu/node1/resources/cassandra/bin/nodetool status

The UN indicates UP NORMAL meaning the node is ready to go. Load indicates current
disk space usage. Owns indicates how many tokens this node is responsible for (it is the
only node in the ring at the moment). Token should be 0 (the same value set in the
cassandra.yaml file). We discuss racks later in the course.

In cqlsh:

CREATE KEYSPACE killrvideo
WITH replication = {'class': 'SimpleStrategy',
'replication_factor': 1 };

USE killrvideo;
CREATE TABLE videos (
 id uuid,
 added_date timestamp,
 title text,
 PRIMARY KEY ((id))
);
COPY videos(id, added_date, title)
FROM '/home/ubuntu/labwork/data-files/videos.csv'
WITH HEADER=TRUE;
CREATE TABLE videos_by_tag (
 tag text,
 video_id uuid,
 added_date timestamp,
 title text,
 PRIMARY KEY ((tag), added_date, video_id))
 WITH CLUSTERING ORDER BY(added_date DESC);

SELECT token(tag), tag
FROM videos_by_tag;

You can refresh your memory as to which nodes own which token ranges by running the
following in the terminal:
/home/ubuntu/node1/resources/cassandra/bin/nodetool ring

To display what node owns the partition:
ubuntu@ds201-node1:~$ /home/ubuntu/node1/resources/cassandra/bin/nodetool getendpoints killrvideo videos_by_tag 'cassandra'
127.0.0.2

getendpoints returns the IP addresses of the node(s) which store the partitions with
the respective partition key value (the last argument in single quotes: cassandra and
datastax respectively). Notice we must also supply the keyspace and table name we
are interested in since we set replication on a per-keyspace basis. There is more on
replication to come later in this course.

rm -rf /home/ubuntu/node1/data/

edit cassandra.yaml
set num_tokens = 128
comment out initial_token

restart cassandra


display all nodes and their partition tag initial value:

/home/ubuntu/node1/resources/cassandra/bin/nodetool ring


When using vnodes, cassandra auto assign token ranges.

/home/ubuntu/node1/resources/cassandra/bin/nodetool gossipinfo

Start cqlsh and execute the following query of the system.peers table which stores
some gossip data about a node's peers.
SELECT peer, data_center, host_id, preferred_ip, rack,
release_version, rpc_address, schema_version
FROM system.peers;
Notice the values here are some of the same values you saw in the terminal. Also notice
that a node does not store a row of peer data for itself. By default, cqlsh connects to
127.0.0.1

Clear nodes:

1- stop nodes
2- 

rm -rf /home/ubuntu/node1/data
rm -rf /home/ubuntu/node2/data

) Edit /home/ubuntu/node1/resources/cassandra/conf/cassandra.yaml and
find the endpoint_snitch setting.
NOTE: Using endpoint_snitch default DseSimpleSnitch places your node in a
datacenter that is based upon work type
# You can use a custom Snitch by setting this to the full class name
# of the snitch, which will be assumed to be on your classpath.
endpoint_snitch: com.datastax.bdp.snitch.DseSimpleSnitch

5) Change the endpoint_snitch to GossipingPropertyFileSnitch.
# You can use a custom Snitch by setting this to the full class name
# of the snitch, which will be assumed to be on your classpath.
endpoint_snitch: GossipingPropertyFileSnitch

) Edit /home/ubuntu/node1/resources/cassandra/conf/cassandrarackdc.properties file.
# These properties are used with GossipingPropertyFileSnitch and will
# indicate the rack and dc for this node
dc=dc1
rack=rack1
This is the file that the GossipingPropertyFileSnitch uses to determine the rack
and data center this particular node belongs to.
NOTE: Racks and datacenters are purely logical assignments to Apache Cassandra™. You
will want to ensure that your logical racks and data centers align with your physical
failure zones.

1) In your terminal, extract the tarball again to make a third node by executing the
following commands (be sure you are in the /home/ubuntu/ directory).
tar -xf dse-6.0.0-bin.tar.gz
mv dse-6.0.0 node3
labwork/config_node 3

Edit /home/ubuntu/node3/resources/cassandra/conf/cassandra.yaml.
Change num_tokens to 128 and comment out initial_token.

4) Change the endpoint_snitch to GossipingPropertyFileSnitch. Save your
changes and exit the editor.

Now we will re-import our KillVideo data. Open cqlsh. Execute the following CQL
CREATE KEYSPACE statement to use NetworkTopologyStrategy with replication set
to store one replica per data center:
CREATE KEYSPACE killrvideo
WITH replication = {
 'class': 'NetworkTopologyStrategy',
 'east-side': 1,
 'west-side': 1
};


Now let's recreate our videos_by_tag table and re-import the data. Execute the
following commands:
CREATE TABLE videos_by_tag (
 tag text,
 video_id uuid,
 added_date timestamp,
 title text,
 PRIMARY KEY ((tag), added_date, video_id))
 WITH CLUSTERING ORDER BY (added_date DESC);
COPY videos_by_tag(tag, video_id, added_date, title)
FROM '/home/ubuntu/labwork/data-files/videos-by-tag.csv'
WITH HEADER=TRUE;

11) Let's determine which nodes our replicas ended up on. Execute the following commands
in the terminal:
/home/ubuntu/node1/resources/cassandra/bin/nodetool getendpoints killrvideo
videos_by_tag 'cassandra'
/home/ubuntu/node1/resources/cassandra/bin/nodetool getendpoints killrvideo
videos_by_tag 'datastax'
NOTE: nodetool displays the IP addresses of the nodes containing our data.
Notice Apache Cassandra™ stores each replica twice, and each replica is in a different
data center. Your results may vary due to randomness in choosing tokens for vnodes.

Now check the current consistency level by executing the CONSISTENCY command as we
have it written out for you here:
CONSISTENCY;

Notice that consistency is ONE; meaning only one node must acknowledge a write on a
write request, and only one node must return a result set to satisfy a read request.

Set your consistency level to TWO by executing the following command:
CONSISTENCY TWO;
NOTE: In this case, TWO is the same as ALL because our current replication settings store
one replica per data center, and we have two data centers.

 Determine which nodes hold the replicas for the cassandra partition tag value in the
videos_by_tag table by executing the following command at the terminal prompt:
/home/ubuntu/node1/resources/cassandra/bin/nodetool getendpoints
killrvideo videos_by_tag 'cassandra'

UPDATE killrvideo.videos_by_tag
SET title = 'Me LovEEEEEEEE Cassandra'
WHERE tag = 'cassandra' AND added_date = '2016-02-08' AND
video_id = paste_your_video_id;

5) Change your consistency level to ANY:
CONSISTENCY ANY;
NOTE: When a client writes with Consistency ANY, storing a hint is sufficient for the
write to be successful. Consistency ONE requires that at least one successful node
write - a hint is not sufficient.
6) Execute the following INSERT command:
INSERT INTO videos_by_tag(tag, added_date, video_id, title)
VALUES ('cassandra', '2016-2-11', uuid(), 'Cassandra, Take Me
Home');
The write will succeed even though both replica nodes are down. The one and only node
left in the cluster stores the writes as hints for the two replica nodes until they come
back online.

You can see that the single node (i.e., the node still running) stored a hint by navigating
to its /home/ubuntu/nodeX/data/hints/ directory.

) Choose one of the nodes to bring down. Before bringing the node down, flush its data
by executing the following command:
/home/ubuntu/nodeX/resources/cassandra/bin/nodetool drain

) In cqlsh, execute the query that follows. What do you think the results will be?
SELECT *
FROM killrvideo.videos_by_tag
WHERE tag = 'cassandra';
The result is empty because we deleted the data files on that node for
videos_by_tag.
11) Bring up the other downed node. Wait for it to come online. Keep track of which node
this is as we will take it down again shortly after triggering a read repair. Remember that
this node you are bringing up still has all our data for the cassandra partition.
12) In cqlsh, set your consistency level to two.
CONSISTENCY TWO;
A consistency level of two will cause Cassandra to read both replicas, perform the
checksum diff, and notice the data is not in sync between the two nodes.
Apache Cassandra™ will then invoke a read repair to repair the node from which we
deleted the data.

Investigate the commit-log directory.
ls -lh /home/ubuntu/node/data/commit-log

6) Let's put a watch on this directory to see how it changes as we write data to
Apache Cassandra™. Open a second terminal and ssh to the remote machine.
7) In the new terminal, execute the following command:
watch -n 1 -d "ls -lh /home/ubuntu/node/data/commit-log"
NOTE: To exit the watch later, press CTRL-C

) We will now use the cassandra-stress tool to write several thousand records to our
node. Execute the following command in your original terminal:
/home/ubuntu/node/resources/cassandra/tools/bin/cassandra-stress
write no-warmup n=250000 -port native=9041 -rate threads=1
Be sure your second terminal is also visible as cassandra-stress executes.
cassandra-stress will write 250,000 rows to your node.
There are a few things to watch out for while cassandra-stress inserts keys:
o The total size will continue to increase.
o The timestamp will change for the current segment being written.
o You may get additional commit log files as well.

10) Execute the following nodetool command:
/home/ubuntu/node/bin/nodetool cfstats keyspace1.standard1
cassandra-stress created the keyspace1.standard1 table and populated its
data. `cfstats` gives you column family stats. Column family is a deprecated term for a
table.

12) Execute the following nodetool command which will flush the memtable contents to
disk.
/home/ubuntu/node/resources/cassandra/bin/nodetool flush

14) Another simple exercise you can do is shut down your node, delete the
logs/system.log file, restart your node, then search for CommitLog.java in the
new logs/system.log file. You may see lines reporting replays.
If there were no commit log segments found during startup, no replay needs to be done.
If Apache Cassandra™ finds commit log files, it will replay the mutations in those files
into memtables and then flush the memtables to disk.




