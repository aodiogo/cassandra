dse cassandra

/home/ubuntu/node/resources/cassandra/bin/cqlsh

dsetool status

CREATE KEYSPACE killrvideo
WITH replication = {
'class':'SimpleStrategy',
'replication_factor': 1
};

USE killrvideo;

CREATE TABLE videos (
video_id TIMEUUID,
added_date TIMESTAMP,
title TEXT,
PRIMARY KEY (video_id)
);


INSERT INTO videos (video_id, added_date, title)
VALUES (1645ea59-14bd-11e5-a993-8138354b7e31, '2014-01-29', 'Cassandra History');

TRUNCATE videos;


COPY videos(video_id, added_date, title)
FROM '/home/ubuntu/labwork/data-files/videos.csv'
WITH HEADER=TRUE;

/home/ubuntu/node/bin/dsetool status

SELECT token(video_id), video_id
FROM videos;

CREATE TABLE videos_by_tag (
tag TEXT,
video_id UUID,
added_date TIMESTAMP,
title TEXT,
PRIMARY KEY ((tag), video_id)
);

CREATE TABLE videos_by_tag (
tag text,
video_id uuid,
added_date timestamp,
title text,
PRIMARY KEY ((tag), added_date, video_id)
) WITH CLUSTERING ORDER BY(added_date DESC) WITH HEADER = true;

SELECT *
FROM videos_by_tag
ORDER BY added_date ASC;

SELECT *
FROM videos_by_tag
WHERE tag = 'cassandra'
ORDER BY added_date ASC;

SELECT *
FROM videos_by_tag
WHERE tag = 'cassandra' and added_date >= '2013-1-1'
ORDER BY added_date ASC;

# python

#start python interpreter
python

print('{0:12} {1:40} {2:5}'.format('Tag', 'ID', 'Title'))
for val in session.execute("select * from videos_by_tag"):
 print('{0:12} {1:40} {2:5}'.format(val[0], val[2], val[3]))

print('{0:12} {1:40} {2:5}'.format('Tag', 'ID', 'Title'))
for val in session.execute("select * from videos_by_tag"):
 print('{0:12} {1:40} {2:5}'.format(val[0], val[2], val[3]))

nodetool help

nodetool status

nodetool info

nodetool describecluster

nodetool getlogginglevels

./nodetool setlogginglevel org.apache.cassandra TRACE

The command setlogginglevel dynamically changes the logging level used by
Apache Cassandra™ without the need for a restart. You can also look at the
/var/log/cassandra/system.log afterwards to observe the changes.

./nodetool settraceprobability 0.1

The resultant value from the settraceprobability command represents a decimal
describing the percentage of queries being saved, starting from 0 (0%) to 1 (100%). Saved traces
can then be viewed in the system_traces keyspace.

./nodetool drain
The drain command stops writes from occurring on the node and flushes all data to disk.
Typically, this command may be run before stopping an Apache Cassandra™ node.

./nodetool stopdaemon
The stopdaemon command stops a node's execution. Wait for it to complete.

Restart node by running:

/home/ubuntu/node/bin/dse cassandra

We will now stress the node using a simple tool called Apache Cassandra(TM) Stress. Once
your node has restarted, navigate to the
/home/ubuntu/node/resources/cassandra/tools/bin directory in the terminal. Run
cassandra-stress to populate the cluster with 50,000 partitions using 1 client thread and
without any warmup using:
./cassandra-stress write n=50000 no-warmup -rate threads=1


nodetool flush

Initially, we will see a long list of setting for the stress run. As Apache Cassandra™ stress
executes, it logs several statistics to the terminal. Each line displays the statistics for the
operations that occurred each second and shows number of partitions written, operations per
second, latency information, and more.

The flush command commits all written (memtable, discussed later) data to disk. Unlike
drain, flush allows further writes to occur.

inside cqlsh:

DESCRIBE KEYSPACES;

USE keyspace1;

DESCRIBE TABLES;

SELECT *
FROM standard1
LIMIT 5;

Stop the current node
nodetool stopdaemon 

Ring

/home/ubuntu/node2/resources/cassandra/conf/cassandra.yaml

Change initial token to

initial_token: 9223372036854775807

/home/ubuntu/node1/resources/cassandra/bin/nodetool status

The UN indicates UP NORMAL meaning the node is ready to go. Load indicates current
disk space usage. Owns indicates how many tokens this node is responsible for (it is the
only node in the ring at the moment). Token should be 0 (the same value set in the
cassandra.yaml file). We discuss racks later in the course.

In cqlsh:

CREATE KEYSPACE killrvideo
WITH replication = {'class': 'SimpleStrategy',
'replication_factor': 1 };

USE killrvideo;
CREATE TABLE videos (
 id uuid,
 added_date timestamp,
 title text,
 PRIMARY KEY ((id))
);
COPY videos(id, added_date, title)
FROM '/home/ubuntu/labwork/data-files/videos.csv'
WITH HEADER=TRUE;
CREATE TABLE videos_by_tag (
 tag text,
 video_id uuid,
 added_date timestamp,
 title text,
 PRIMARY KEY ((tag), added_date, video_id))
 WITH CLUSTERING ORDER BY(added_date DESC);

SELECT token(tag), tag
FROM videos_by_tag;

You can refresh your memory as to which nodes own which token ranges by running the
following in the terminal:
/home/ubuntu/node1/resources/cassandra/bin/nodetool ring

To display what node owns the partition:
ubuntu@ds201-node1:~$ /home/ubuntu/node1/resources/cassandra/bin/nodetool getendpoints killrvideo videos_by_tag 'cassandra'
127.0.0.2

getendpoints returns the IP addresses of the node(s) which store the partitions with
the respective partition key value (the last argument in single quotes: cassandra and
datastax respectively). Notice we must also supply the keyspace and table name we
are interested in since we set replication on a per-keyspace basis. There is more on
replication to come later in this course.

rm -rf /home/ubuntu/node1/data/

edit cassandra.yaml
set num_tokens = 128
comment out initial_token

restart cassandra


display all nodes and their partition tag initial value:

/home/ubuntu/node1/resources/cassandra/bin/nodetool ring


When using vnodes, cassandra auto assign token ranges.

/home/ubuntu/node1/resources/cassandra/bin/nodetool gossipinfo

Start cqlsh and execute the following query of the system.peers table which stores
some gossip data about a node's peers.
SELECT peer, data_center, host_id, preferred_ip, rack,
release_version, rpc_address, schema_version
FROM system.peers;
Notice the values here are some of the same values you saw in the terminal. Also notice
that a node does not store a row of peer data for itself. By default, cqlsh connects to
127.0.0.1

Clear nodes:

1- stop nodes
2- 

rm -rf /home/ubuntu/node1/data
rm -rf /home/ubuntu/node2/data

) Edit /home/ubuntu/node1/resources/cassandra/conf/cassandra.yaml and
find the endpoint_snitch setting.
NOTE: Using endpoint_snitch default DseSimpleSnitch places your node in a
datacenter that is based upon work type
# You can use a custom Snitch by setting this to the full class name
# of the snitch, which will be assumed to be on your classpath.
endpoint_snitch: com.datastax.bdp.snitch.DseSimpleSnitch

5) Change the endpoint_snitch to GossipingPropertyFileSnitch.
# You can use a custom Snitch by setting this to the full class name
# of the snitch, which will be assumed to be on your classpath.
endpoint_snitch: GossipingPropertyFileSnitch

) Edit /home/ubuntu/node1/resources/cassandra/conf/cassandrarackdc.properties file.
# These properties are used with GossipingPropertyFileSnitch and will
# indicate the rack and dc for this node
dc=dc1
rack=rack1
This is the file that the GossipingPropertyFileSnitch uses to determine the rack
and data center this particular node belongs to.
NOTE: Racks and datacenters are purely logical assignments to Apache Cassandra™. You
will want to ensure that your logical racks and data centers align with your physical
failure zones.

1) In your terminal, extract the tarball again to make a third node by executing the
following commands (be sure you are in the /home/ubuntu/ directory).
tar -xf dse-6.0.0-bin.tar.gz
mv dse-6.0.0 node3
labwork/config_node 3

Edit /home/ubuntu/node3/resources/cassandra/conf/cassandra.yaml.
Change num_tokens to 128 and comment out initial_token.

4) Change the endpoint_snitch to GossipingPropertyFileSnitch. Save your
changes and exit the editor.

Now we will re-import our KillVideo data. Open cqlsh. Execute the following CQL
CREATE KEYSPACE statement to use NetworkTopologyStrategy with replication set
to store one replica per data center:
CREATE KEYSPACE killrvideo
WITH replication = {
 'class': 'NetworkTopologyStrategy',
 'east-side': 1,
 'west-side': 1
};


Now let's recreate our videos_by_tag table and re-import the data. Execute the
following commands:
CREATE TABLE videos_by_tag (
 tag text,
 video_id uuid,
 added_date timestamp,
 title text,
 PRIMARY KEY ((tag), added_date, video_id))
 WITH CLUSTERING ORDER BY (added_date DESC);
COPY videos_by_tag(tag, video_id, added_date, title)
FROM '/home/ubuntu/labwork/data-files/videos-by-tag.csv'
WITH HEADER=TRUE;

11) Let's determine which nodes our replicas ended up on. Execute the following commands
in the terminal:
/home/ubuntu/node1/resources/cassandra/bin/nodetool getendpoints killrvideo
videos_by_tag 'cassandra'
/home/ubuntu/node1/resources/cassandra/bin/nodetool getendpoints killrvideo
videos_by_tag 'datastax'
NOTE: nodetool displays the IP addresses of the nodes containing our data.
Notice Apache Cassandra™ stores each replica twice, and each replica is in a different
data center. Your results may vary due to randomness in choosing tokens for vnodes.

Now check the current consistency level by executing the CONSISTENCY command as we
have it written out for you here:
CONSISTENCY;

Notice that consistency is ONE; meaning only one node must acknowledge a write on a
write request, and only one node must return a result set to satisfy a read request.

Set your consistency level to TWO by executing the following command:
CONSISTENCY TWO;
NOTE: In this case, TWO is the same as ALL because our current replication settings store
one replica per data center, and we have two data centers.

 Determine which nodes hold the replicas for the cassandra partition tag value in the
videos_by_tag table by executing the following command at the terminal prompt:
/home/ubuntu/node1/resources/cassandra/bin/nodetool getendpoints
killrvideo videos_by_tag 'cassandra'

UPDATE killrvideo.videos_by_tag
SET title = 'Me LovEEEEEEEE Cassandra'
WHERE tag = 'cassandra' AND added_date = '2016-02-08' AND
video_id = paste_your_video_id;


